# ğŸ¥‘ Solution For Argmax's Search By Ingredients Challenge By **Guy Vitelson**

---
##### Ping me via ğŸ”— **[Linkedin](https://www.linkedin.com/in/guyvitelson/)**  ğŸ™ **[GitHub](https://github.com/v1t3ls0n)**  âœ‰ï¸ **[Mail](mailto:guyvitelson@gmail.com)**
---

## ğŸ§­ Project Overview

This pipeline implements two independent binary classification tasks for recipes:

- **Keto-Friendly**: â‰¤ 10 g net carbohydrates per 100 g serving
- **Vegan**: no animal-derived ingredients (strictly plant-based)

We assume **no labeled data is available**, and solve the task using weak supervision, rule-based logic, and machine learning. We go beyond requirements by integrating:

- âœ… USDA FoodData Central for authoritative nutritional validation AND training data augmentation
- âœ… Multi-stage silver labeling with nutritional data + regex + whitelist overrides + fuzzy matching
- âœ… ML model training over sparse text/image features  
- âœ… 4-level hierarchical ensemble architecture with dynamic weighting
- âœ… Comprehensive memory management and crisis handling
- âœ… Multi-threaded image downloading with error categorization
- âœ… CLI + Docker + logging + caching + restart loop prevention

---

## ğŸš€ Quick Start (2 minutes)

```bash
# 1. Clone and run
git clone https://github.com/v1t3ls0n/search_by_ingredients_v1t3ls0n.git
cd search_by_ingredients_v1t3ls0n
docker-compose up -d

# 2. Test via CLI (classifiers are in the web container)
docker-compose exec web python3 /app/web/diet_classifiers.py --ingredients "almond flour, eggs, butter"

# 3. View web interface  
Open http://localhost:8080 in your browser

# 4. The trained models are already included - no training required!
```

---

## âš™ï¸ Pipeline Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Raw Recipe Dataset  â”‚ (No labels)
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ USDA Nutritional DB â”‚ (Thousands of ingredients with carb content)
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â–¼
Silver Label Generator 
â”œâ”€ Applies multi-stage rules to recipe data
â”œâ”€ Adds USDA entries as new training rows
â””â”€ Creates extended silver dataset
     â–¼
Extended Silver Dataset (recipes + USDA)
     â–¼
Text Vectorizer (TF-IDF)
     â”‚
     â”œâ”€â”€â–¶ Optional: Image Embeddings (ResNet-50)
     â–¼
Individual Model Training (Text / Image / Hybrid)
     â–¼
4-Level Hierarchical Ensemble System
â”œâ”€ Level 1: Individual Models (15+ algorithms)
â”œâ”€ Level 2: Domain Ensembles (top_n optimization)
â”œâ”€ Level 3: Cross-Domain Blending (alpha optimization)
â””â”€ Level 4: Global Configuration Search
     â–¼
Dynamic Per-Row Weighting + Rule Verification
     â–¼
Export: Metrics, Plots, Artifacts 
```

---

## ğŸ” Silver Labeling System

Our silver labeling engine applies **progressive rule-based heuristics** combined with **USDA nutritional data** to simulate expert knowledge for labeling unlabeled recipes.

### Pre-filtering
Before applying classification logic, we pre-filter unqualified samples:

* âŒ **Photo Filtering:** Remove rows whose `photo_url` contains `nophoto`, `nopic`, or `nopicture`
* âŒ **Empty or malformed ingredient entries** are excluded from training
* âœ… **Ingredient normalization** includes lemmatization, unit removal, numeric stripping, and character simplification

### Multi-Stage Classification Pipeline

The actual classification implements a sophisticated cascade:

#### For Keto Classification:
```python
1. Whitelist Override (Highest Priority)
   - Regex patterns like r"\balmond flour\b", r"\bcoconut flour\b"
   - Immediate acceptance for known keto ingredients

2. USDA Nutritional Check - Whole Phrase
   - Exact lookup in USDA database
   - If found and â‰¤10g carbs/100g â†’ keto-friendly

2b. USDA Token-level Fallback with Fuzzy Matching
   - Individual token lookup with 90% similarity threshold
   - Uses RapidFuzz for intelligent matching
   - Skips common stop words

3. Regex Blacklist (Fast Pattern Matching)
   - Compiled regex patterns from NON_KETO list
   - Uses word boundaries for accurate matching

4. Token-level Blacklist Analysis
   - Additional verification using tokenized ingredients
   - Handles multi-word ingredients like "kidney beans"

5. ML Model Prediction (If Available)
   - Uses trained models with probability output

6. Rule Verification (Final Override)
   - Ensures dietary safety with rule-based corrections
```

#### For Vegan Classification:
```python
1. Whitelist Override
   - Regex patterns handling edge cases like r"\beggplant\b" (not "egg")
   - r"\bbutternut\b" (not "butter")

2. Regex Blacklist Check
   - Comprehensive animal product detection using compiled patterns

3. ML Model with Verification
   - Trained models with rule-based correction
```

### USDA Dataset Integration

The USDA integration is more comprehensive than just validation:

```python
# Actual implementation:
usda_labeled = label_usda_keto_data(carb_df)
silver_txt = pd.concat([silver_txt, usda_labeled], ignore_index=True)
silver_txt.to_csv("artifacts/silver_extended.csv", index=False)
```

- **New training samples**: Thousands of USDA ingredients added as training rows
- **Automatic labeling**: Each USDA ingredient with â‰¤10g carbs/100g â†’ `keto = 1`
- **Dataset augmentation**: Significantly expands training diversity
- **Fuzzy matching**: 90% similarity threshold for ingredient matching

---

## ğŸ§  ML Models and Ensemble

### Text-only classifiers
* **Softmax_TEXT** (Logistic Regression)
* **Ridge_TEXT** (RidgeClassifier)
* **PA_TEXT** (Passive-Aggressive Classifier)
* **SGD_TEXT** (SGDClassifier)
* **NB_TEXT** (Multinomial Naive Bayes)

### Image-only classifiers
* **RF_IMAGE** (Random Forest on ResNet-50 embeddings)
* **LGBM_IMAGE** (LightGBM on ResNet-50 embeddings)
* **MLP_IMAGE** (Multi-layer Perceptron on ResNet-50 embeddings)

### Hybrid (Text + Image) classifiers
* **Softmax_BOTH** (Logistic Regression on concatenated features)
* **Ridge_BOTH** (RidgeClassifier on concatenated features)
* **PA_BOTH** (Passive-Aggressive on concatenated features)
* **RF_BOTH** (Random Forest on concatenated features)
* **LGBM_BOTH** (LightGBM on concatenated features)
* **NB_BOTH** (Naive Bayes on concatenated features)
* **MLP_BOTH** (MLP on concatenated features)
* **TxtImg** (custom textâ€“image fusion model)

### Additional Models

* **Rule** (Pure rule-based classifier used as baseline and fallback)
  - Implements the complete classification pipeline without ML
  - Used when ML models fail or as a comparison baseline
  - Appears in results as "Rule_TEXT" or "Rule_[DOMAIN]"

---

## ğŸ¯ Advanced Ensemble Architecture

### Hierarchical Ensemble Pipeline

```python
# Level 1: Individual Model Results (shown in results tables below)
individual_models = train_all_models()

# Level 2: Domain-Specific Ensembles
text_ensemble = top_n(task="keto", models=text_models, n=3)
image_ensemble = top_n(task="keto", models=image_models, n=2)

# Level 3: Cross-Domain Blending  
blended_ensemble = best_two_domains(
    text_results=text_ensemble,
    image_results=image_ensemble,
    alpha=0.6  # Optimal weight found via grid search
)

# Level 4: Global Optimization
final_ensemble = best_ensemble(
    task="keto",
    ensemble_sizes=[1,2,3,4,5],
    alpha_values=[0.25, 0.5, 0.75]
)
```

### Advanced Model Training Pipeline

The system implements **sophisticated ML training** with production-grade features:

#### **Class Imbalance Handling**
```python
def apply_smote(X, y, max_dense_size: int = int(5e7)):
    # Apply SMOTE only if minority class < 40% of dataset
    counts = np.bincount(y)
    ratio = counts.min() / counts.sum()
    
    if ratio < 0.4:  # Threshold for applying oversampling
        if hasattr(X, "toarray") and X.shape[0] * X.shape[1] > max_dense_size:
            # Too large for SMOTE - use random oversampling
            ros = RandomOverSampler(random_state=42)
            return ros.fit_resample(X, y)
        else:
            # Use SMOTE for synthetic minority examples
            smote = SMOTE(sampling_strategy=0.3, random_state=42)
            return smote.fit_resample(X, y)
    return X, y
```

**Intelligent Oversampling Strategy:**
- **Automatic Detection**: Only applies when minority class < 40%
- **SMOTE**: Creates synthetic examples for balanced learning
- **Random Oversampling**: Fallback for very large sparse matrices
- **Memory Aware**: Switches strategies based on matrix size

#### **Comprehensive Hyperparameter Tuning**
```python
HYPER = {
    # Text models
    "Softmax": {"C": [0.1, 1, 10]},
    "SGD": {"alpha": [1e-4, 1e-3], "loss": ["log_loss", "modified_huber"]},
    "Ridge": {"alpha": [0.1, 1.0, 10.0]},
    "PA": {"C": [0.5, 1.0]},
    
    # Image/mixed models  
    "MLP": {
        "hidden_layer_sizes": [(256,), (512, 128)],
        "alpha": [0.0001, 0.001],
        "learning_rate_init": [0.001, 0.005]
    },
    "RF": {
        "n_estimators": [150, 300],
        "max_depth": [None, 20],
        "min_samples_leaf": [1, 2]
    },
    "LGBM": {
        "learning_rate": [0.05, 0.1],
        "num_leaves": [31, 63],
        "n_estimators": [150, 250]
    }
}
```

**Advanced Tuning Features:**
- **Grid Search with Cross-Validation**: Exhaustive parameter exploration
- **Early Stopping**: `tune_with_early_stopping()` for large parameter spaces  
- **Intelligent Caching**: Results cached in `BEST` dictionary
- **Model-Specific Grids**: Optimized parameters per algorithm type
- **Fallback Handling**: Graceful degradation to default parameters

### Dynamic Ensemble Features

The ensemble system implements a **4-level hierarchical architecture** with sophisticated optimizations:

#### **Level 1: Individual Model Training**
- 15+ diverse models across text/image/hybrid domains
- Automatic probability calibration via `ensure_predict_proba()`
- Cross-validation and composite metric scoring

#### **Level 2: Domain-Specific Ensembles** 
- **`top_n()`**: Creates optimal ensembles within each domain (text/image)
- **Soft voting classifier** with fallback to manual probability averaging
- **Composite scoring**: Weighted combination of 6 metrics (F1, Precision, Recall, ROC-AUC, PR-AUC, Accuracy)
- **Greedy ensemble building**: Tests ensemble sizes 1â†’N for optimal configuration

#### **Level 3: Cross-Domain Blending**
- **`best_two_domains()`**: Blends best text ensemble with best image ensemble
- **Alpha optimization**: Grid search over blending weights (0.25, 0.5, 0.75)
- **Per-row dynamic weighting**: Image models get zero weight for text-only rows

#### **Level 4: Global Optimization**
- **`best_ensemble()`**: Exhaustive search across ensemble configurations
- **Multi-objective optimization**: Balances performance vs complexity
- **Intelligent fallbacks**: Handles missing modalities gracefully

### Dynamic Weighting Strategy

**Per-Row Adaptation:**
```python
def dynamic_ensemble(estimators, X_gold, gold, task):
    # For each row individually:
    if row.has_text and row.has_image:
        weights = [0.6, 0.4]  # text_ensemble, image_ensemble
    elif row.has_text_only:
        weights = [1.0, 0.0]  # text_ensemble only
    elif row.has_image_only:
        weights = [0.0, 1.0]  # image_ensemble only
    
    return weighted_prediction(weights, ensembles)
```

### Composite Scoring System

Models ranked using weighted combination of 6 metrics:
- **F1 Score** (1/6): Primary classification metric
- **Precision** (1/6): False positive control
- **Recall** (1/6): False negative control  
- **ROC-AUC** (1/6): Ranking quality
- **PR-AUC** (1/6): Precision-recall trade-off
- **Accuracy** (1/6): Overall correctness

```python
composite_score = (F1 + Precision + Recall + ROC_AUC + PR_AUC + Accuracy) / 6
```

---

## ğŸ§ª Model Evaluation Results
**Image models trained on ~70K images, all models evaluated on the gold set (ground_truth data)**

### ğŸ¥‘ **Keto Models** (Sorted by F1)

| ğŸ§  Model         | ğŸ¯ Task | âœ… Accuracy | ğŸ¯ Precision | ğŸ” Recall | ğŸ† F1-Score | â±ï¸ Time (s) |
| ---------------- | ------- | ---------- | ------------ | --------- | ----------- | ----------- |
| ğŸ¤– Softmax_TEXT  | keto    | 0.970      | 0.951        | 0.975     | **0.963**   | 3.6         |
| ğŸ§  Ridge_TEXT    | keto    | 0.970      | 0.951        | 0.975     | **0.963**   | 12.4        |
| âš”ï¸ PA_TEXT       | keto    | 0.970      | 0.951        | 0.975     | **0.963**   | 4.2         |
| ğŸ§ª SGD_TEXT      | keto    | 0.970      | 0.951        | 0.975     | **0.963**   | 0.6         |
| ğŸ§  TxtImg        | keto    | 0.940      | 0.930        | 0.960     | 0.950       | â€“           |
| ğŸ¦  NB_TEXT       | keto    | 0.960      | 0.950        | 0.950     | 0.950       | 0.3         |
| ğŸŒ² RF_IMAGE      | keto    | 0.942      | 0.929        | 0.963     | 0.945       | 111.5       |
| ğŸ§¬ Softmax_BOTH  | keto    | 0.942      | 0.929        | 0.963     | 0.945       | 85.8        |
| ğŸŒŸ LGBM_BOTH     | keto    | 0.942      | 0.929        | 0.963     | 0.945       | 109.1       |
| âš¡ LGBM_IMAGE     | keto    | 0.904      | 0.923        | 0.889     | 0.906       | 87.7        |
| ğŸ NB_BOTH       | keto    | 0.865      | 0.955        | 0.778     | 0.857       | 0.2         |
| ğŸ§  MLP_IMAGE     | keto    | 0.750      | 0.938        | 0.556     | 0.698       | 119.8       |

### ğŸŒ± **Vegan Models** (Sorted by F1)

| ğŸ§  Model         | ğŸ¯ Task | âœ… Accuracy | ğŸ¯ Precision | ğŸ” Recall | ğŸ† F1-Score | â±ï¸ Time (s) |
| ---------------- | ------- | ---------- | ------------ | --------- | ----------- | ----------- |
| ğŸŒ² RF_IMAGE      | vegan   | 1.000      | 1.000        | 1.000     | **1.000**   | 77.6        |
| ğŸ§¬ Softmax_BOTH  | vegan   | 1.000      | 1.000        | 1.000     | **1.000**   | 73.2        |
| ğŸŒ² RF_BOTH       | vegan   | 1.000      | 1.000        | 1.000     | **1.000**   | 29.0        |
| ğŸŒŸ LGBM_BOTH     | vegan   | 1.000      | 1.000        | 1.000     | **1.000**   | 127.9       |
| ğŸ§  TxtImg        | vegan   | 1.000      | 1.000        | 1.000     | **1.000**   | â€“           |
| ğŸ§  Ridge_BOTH    | vegan   | 0.981      | 1.000        | 0.964     | **0.982**   | 462.9       |
| âš”ï¸ PA_BOTH       | vegan   | 0.981      | 1.000        | 0.964     | **0.982**   | 25.9        |
| ğŸ¤– Softmax_TEXT  | vegan   | 0.980      | 0.975        | 0.975     | **0.975**   | 1.6         |
| âš”ï¸ PA_TEXT       | vegan   | 0.980      | 0.975        | 0.975     | **0.975**   | 4.5         |
| ğŸ§  MLP_BOTH      | vegan   | 0.962      | 1.000        | 0.929     | 0.963       | 2515.7      |
| âš¡ LGBM_IMAGE     | vegan   | 0.962      | 1.000        | 0.929     | 0.963       | 54.3        |
| ğŸ§  Ridge_TEXT    | vegan   | 0.970      | 0.974        | 0.950     | 0.962       | 12.2        |
| ğŸ§ª SGD_TEXT      | vegan   | 0.970      | 0.974        | 0.950     | 0.962       | 0.6         |
| ğŸ¦  NB_TEXT       | vegan   | 0.960      | 0.974        | 0.925     | 0.949       | 0.1         |
| ğŸ NB_BOTH       | vegan   | 0.788      | 1.000        | 0.607     | 0.756       | 0.2         |
| ğŸ§  MLP_IMAGE     | vegan   | 0.596      | 1.000        | 0.250     | 0.400       | 100.6       |

## ğŸ§  Key Takeaways from Results

These results demonstrate **exceptionally strong performance**, especially considering:

* The **entire training pipeline is weakly supervised** (no ground-truth labels during training)
* **Individual models** achieve F1-scores of **0.96+** for keto classification
* **Hierarchical ensembles** (not shown in tables) further improve performance through:
  - Domain-specific ensemble optimization (`top_n()`)
  - Cross-modal blending (`best_two_domains()`) 
  - Global configuration search (`best_ensemble()`)
* **Vegan models reached perfect classification (F1 = 1.0)** with image+text ensembles
* The integration of **USDA nutritional data** provides science-based keto classifications
* Image models trained on **70,000 images** show the benefit of scale

**Note:** Results tables show individual model performance. The production system uses sophisticated ensemble methods that combine these models for improved accuracy.

---

## ğŸ–¥ï¸ CLI & Docker Interface

### ğŸ§ª Direct Python Execution

```bash
# ğŸ”§ Train and evaluate on silver + gold sets
python diet_classifiers.py --train --mode both

# ğŸ“Š Evaluate trained models on a gold-labeled test set
python diet_classifiers.py --ground_truth /usr/src/data/ground_truth_sample.csv

# ğŸ´ Classify a custom list of ingredients
python diet_classifiers.py --ingredients "almond flour, coconut oil, cocoa powder"

# ğŸ“ Predict labels for an unlabeled CSV file (batch inference)
python diet_classifiers.py --predict /path/to/recipes.csv
```

### ğŸ³ Dockerized Execution

```bash
scripts/train.sh                # Only train models
scripts/eval_ground_truth.sh    # Evaluate trained models on ground_truth.csv
scripts/eval_custom.sh          # Evaluate on user-provided CSV file
scripts/run_full_pipeline.sh    # Train + evaluate (on given ground_truth.csv) + classify ingredients (on toy list "almond flour, erythritol, egg whites") end-to-end
scripts/update_git.sh           # Git commit + push helper
```

### ğŸ”§ Supported CLI Arguments

| Argument         | Type   | Description                                                    |
| ---------------- | ------ | -------------------------------------------------------------- |
| `--train`        | flag   | Run the full training pipeline on silver-labeled data          |
| `--ground_truth` | path   | Evaluate trained models on a gold-labeled CSV                  |
| `--predict`      | path   | Run batch inference on unlabeled CSV file                      |
| `--ingredients`  | string | Comma-separated list or JSON array for classification          |
| `--mode`         | choice | Feature mode: `text`, `image`, or `both` (default: `both`)     |
| `--force`        | flag   | Force re-computation of image embeddings                       |
| `--sample_frac`  | float  | Subsample silver dataset for training (e.g. `0.1` = 10%)       |

---

## ğŸ’¾ Memory Management & Optimization

### Dynamic Memory Monitoring
- Real-time tracking at each pipeline stage
- Threshold-based actions:
  - < 70%: Normal operation
  - 70-85%: Moderate warning with optimization
  - > 85%: High usage triggers aggressive cleanup
- Automatic GPU memory clearing with `torch.cuda.empty_cache()`
- Detailed logging of memory freed and objects collected

### Emergency Memory Crisis Handler
```python
def handle_memory_crisis():
    # 5 aggressive GC passes
    # Complete GPU memory clearing with synchronization
    # Python internal cache invalidation
    # Memory compaction
```

### Sparse Matrix Optimization
- TF-IDF features stored as CSR sparse matrices
- 90%+ memory reduction for text features
- Efficient sparse-dense concatenation for hybrid features

---

## ğŸ–¼ï¸ Image Processing Pipeline

### Multi-threaded Downloading
- ThreadPoolExecutor with up to 16 concurrent workers
- Bandwidth tracking (MB/s for each download)
- Error categorization: Timeout, 404, Forbidden, Invalid Content
- Smart retries with exponential backoff (max 2 attempts)
- Atomic file writing with temporary files

### Image Quality Filtering
```python
def filter_low_quality_images():
    # Remove embeddings with very low variance (blank/corrupted)
    # Remove embeddings too similar to mean (generic/placeholder)
    # Ensures at least 50% retention
```

### GPU Acceleration
- Automatic CUDA detection and fallback to CPU
- Dynamic batch sizing based on GPU memory
- Memory-aware processing to prevent OOM errors

---

## ğŸ›¡ï¸ Advanced Error Handling

### Multi-Layer Fallback Architecture

1. **Model Training Fallbacks**
   - Primary: Grid search with cross-validation
   - Fallback 1: Train with default parameters
   - Fallback 2: Use rule-based model

2. **Feature Extraction Fallbacks**
   - Primary: Use cached embeddings (if available)
   - Fallback 1: Use backup cache
   - Fallback 2: Compute fresh embeddings
   - Fallback 3: Return zero vectors

3. **Ensemble Creation Fallbacks**
   - Primary: Soft voting classifier
   - Fallback: Manual probability averaging

### Comprehensive Error Tracking
- Categorized error logging with timestamps
- Detailed error analysis and reporting
- Saved to `failed_downloads.txt` and `embedding_errors.txt`

### Restart Loop Prevention
```python
# Environment variable tracking prevents infinite loops
restart_count = os.environ.get('PIPELINE_RESTART_COUNT', '0')
if int(restart_count) > 0:
    print(f"âŒ RESTART LOOP DETECTED - STOPPING")
    sys.exit(1)
```

---

## ğŸš€ Performance Optimization

### Intelligent Caching
- Model caching in `BEST` dictionary
- Image embedding caching with metadata
- Vectorizer and model persistence

### Early Stopping for Hyperparameter Search
```python
def tune_with_early_stopping(patience=3, min_improvement=0.001):
    # Stops when no improvement for 'patience' iterations
    # Saves computation by skipping remaining combinations
```

### Dynamic Feature Selection
- Only computes features needed for current mode
- Efficient index alignment for multi-modal data
- Sparse-dense feature combination

---
## ğŸš€ Project Directory Structure

### ğŸ“ Directory Overview

**The complete implementation resides in a single file: `web/src/diet_classifiers.py`**. 
All other files are boilerplate or configuration.

---

### ğŸ¯ Core Implementation

#### â­ **The Heart of the Project**

```
web/src/diet_classifiers.py
```

**This single file contains:**
- âœ… Complete ML pipeline implementation
- âœ… All classification algorithms
- âœ… Data processing logic
- âœ… CLI interface
- âœ… Model training & evaluation
- âœ… Feature engineering
- âœ… Hyperparameter optimization

---

### ğŸ—‚ï¸ Directory Layout

#### ğŸŒ **Web & API Container** (`web/`)
*Contains the actual implementation*

```
web/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ templates/
â”‚   â”‚   â””â”€â”€ index.html            # [Boilerplate - Minimal frontend]
â”‚   â”œâ”€â”€ app.py                    # [Boilerplate - Flask server]
â”‚   â”œâ”€â”€ diet_classifiers.py       # â­ COMPLETE IMPLEMENTATION â­
â”‚   â”œâ”€â”€ index_data.py             # [Boilerplate - OpenSearch indexing]
â”‚   â””â”€â”€ init.sh                   # [Modified - Startup script]
â”œâ”€â”€ Dockerfile                    # [Modified - Container config]
â””â”€â”€ requirements.txt              # [Modified - Dependencies]
```

#### ğŸ¤– **Pre-trained Models** (`pretrained_models/`)
*Model storage*

```
pretrained_models/
â””â”€â”€ models.zip                    # Pre-trained models & vectorizers
```

#### ğŸ“Š **Pipeline Artifacts** (`artifacts/`)
*Created by `diet_classifiers.py` during execution*

```
artifacts/
â”œâ”€â”€ models.pkl                    # Generated by pipeline
â”œâ”€â”€ vectorizer.pkl                # Generated by pipeline
â”œâ”€â”€ silver_extended.csv           # Generated by pipeline
â”œâ”€â”€ eval_metrics.csv              # Generated by pipeline
â”œâ”€â”€ ground_truth_predictions.csv  # Generated by pipeline
â”œâ”€â”€ pipeline.log                  # Generated by pipeline
â””â”€â”€ best_hyperparams.json         # Generated by pipeline
```

#### ğŸ”§ **Configuration & Scripts**

##### ğŸ³ Docker Configuration
```
â”œâ”€â”€ docker-compose.yml            # [Modified - Two-service architecture]
```

##### ğŸ“œ Execution Scripts
*Shell wrappers that invoke `diet_classifiers.py`*
```
scripts/
â”œâ”€â”€ train.sh                      # Calls diet_classifiers.py --train
â”œâ”€â”€ eval_ground_truth.sh          # Calls diet_classifiers.py --eval
â”œâ”€â”€ eval_custom.sh                # Calls diet_classifiers.py --eval-custom
â”œâ”€â”€ run_full_pipeline.sh          # Calls diet_classifiers.py --full
â””â”€â”€ update_git.sh                 # Git helper
```

##### ğŸ“„ Documentation & Version Control
```
â”œâ”€â”€ README.md                     # Project documentation
â”œâ”€â”€ .gitattributes               # Git LFS configuration
â””â”€â”€ .gitignore                   # Version control exclusions
```

---

### ğŸ“ Implementation Notes

#### Streamlined Architecture
The project uses a **two-service Docker architecture** (the original `nb/` notebook container has been removed as unnecessary boilerplate):
- **`os`**: OpenSearch for recipe indexing and search capabilities
- **`web`**: Complete ML pipeline, Flask API, and CLI interface

#### Modified Files (Minor Adjustments)
As per task requirements, only minimal changes were made to:
- ğŸ³ **`/web/Dockerfile`** - Container configuration with dataset downloads
- ğŸ”§ **`/docker-compose.yml`** - Simplified two-service orchestration
- ğŸ“¦ **`/web/requirements.txt`** - ML pipeline dependencies
- ğŸ“„ **`/web/src/init.sh`** - Startup script (model extraction moved to Dockerfile)

#### Untouched Boilerplate
All other files remain as provided in the original boilerplate, ensuring compatibility with the existing infrastructure while removing unnecessary complexity.

---

## âœ… Feature Matrix

| Feature                           | Status | Notes                                      |
| --------------------------------- | ------ | ------------------------------------------ |
| SMOTE class balancing             | âœ…     | Applied automatically when minority < 40%   |
| Grid search hyperparameter tuning| âœ…     | Model-specific parameter grids with CV      |
| Early stopping optimization      | âœ…     | Prevents overfitting in parameter search    |
| Automatic probability calibration| âœ…     | Ensures all models provide probabilities    |
| Model persistence and caching    | âœ…     | Saves/loads models and vectorizers          |
| Restart loop prevention          | âœ…     | Environment variable tracking               |
| Weak supervision via rules        | âœ…     | Multi-stage cascade with whitelists        |
| USDA dataset augmentation         | âœ…     | Adds thousands of training examples        |
| Fuzzy matching for ingredients    | âœ…     | 90% similarity threshold with RapidFuzz    |
| Whitelist override system         | âœ…     | Handles edge cases intelligently           |
| Image + Text dual domain          | âœ…     | Optional multimodal with ResNet-50         |
| Image quality filtering           | âœ…     | Variance and mean-based filtering          |
| 4-level hierarchical ensembles    | âœ…     | Domain â†’ Cross-domain â†’ Global optimization |
| Dynamic per-row weighting         | âœ…     | Adaptive weights based on data availability |
| Memory management & crisis handling| âœ…     | Multi-level optimization with GPU support  |
| Parallel image downloading        | âœ…     | Multi-threaded with error categorization   |
| Comprehensive error tracking      | âœ…     | Categorized logging with detailed reports  |
| Caching + restarts                | âœ…     | Backups + smart reuse                      |
| Evaluation plots + exports        | âœ…     | ROC, PR, Confusion Matrix, CSV             |
| Atomic file operations            | âœ…     | Prevents corruption during writes          |
| Batch inference support           | âœ…     | Via --predict for CSV files                |
| Full container setup              | âœ…     | docker-compose 2-service setup             |
| CLI usage                         | âœ…     | Command-line friendly                      |
| API readiness                     | âœ…     | Flask entrypoint included                  |
| Logging and debugging             | âœ…     | Hierarchical progress bars + structured logs|
| Integration of external knowledge | âœ…     | USDA nutrition database                    |

---

## ğŸ—ï¸ Technical Architecture Deep Dive

### Key Implementation Details

#### Memory Management & Optimization

The system implements sophisticated memory management:

- **`optimize_memory_usage()`**: Multi-level memory optimization with detailed tracking
- **`handle_memory_crisis()`**: Emergency recovery with 5 GC passes and GPU clearing
- **Sparse Matrix Usage**: 90%+ memory reduction for TF-IDF features

#### Parallel Processing

- **Multi-threaded Image Downloads**: ThreadPoolExecutor with 16 workers
- **GPU Acceleration**: Automatic CUDA detection and dynamic batch sizing
- **Parallel Model Training**: All scikit-learn models use `n_jobs=-1`

#### Error Handling

Multi-layer fallback architecture:
- Model training: GridSearch â†’ Default params â†’ Rule-based model
- Feature extraction: Fresh computation â†’ Cached â†’ Backup â†’ Zero vectors
- Ensemble: Soft voting â†’ Manual averaging

#### Performance Optimizations

- **Intelligent Caching**: Models cached in `BEST` dictionary
- **Early Stopping**: `tune_with_early_stopping()` saves computation
- **Dynamic Feature Selection**: Only compute needed features
- **Batch Processing**: Dynamic sizing based on operation type

#### **Advanced ML Pipeline Features**

- **Silver Label Generation**: 6-stage cascade with USDA integration
- **SMOTE Class Balancing**: Applied automatically when minority class < 40%
- **Grid Search Hyperparameter Tuning**: Model-specific parameter grids with cross-validation
- **Early Stopping Optimization**: Prevents overfitting in hyperparameter search  
- **Automatic Probability Calibration**: `ensure_predict_proba()` wraps models without probability outputs
- **Model Diversity**: 15+ algorithms across linear, tree-based, neural network families
- **Hierarchical Ensembles**: 4-level optimization with dynamic weighting
- **Ensemble Optimization**: Greedy selection with composite scoring

#### **Production Features**

- **Restart Loop Prevention**: Environment variable tracking prevents infinite restart cycles
- **Comprehensive Logging**: Multi-handler with structured output
- **Progress Tracking**: Hierarchical tqdm progress bars
- **Atomic File Operations**: Temporary files with integrity checks
- **Performance Metrics**: Detailed tracking of timing and resources
- **Memory Crisis Management**: `handle_memory_crisis()` with 5-pass garbage collection
- **Model Persistence**: Automatic saving/loading of trained models and vectorizers
- **Graceful Error Handling**: Multi-level fallbacks for all pipeline components

#### Implementation Patterns

- **Lazy Loading**: Global caches for datasets and USDA data
- **`build_models()`**: Flexible model construction based on task/domain

---

## ğŸ“Š Performance Characteristics

| Operation | Throughput | Memory Usage | GPU Benefit |
|-----------|------------|--------------|-------------|
| Text Vectorization | 10K docs/sec | O(vocab_size) | None |
| Image Download | 50-100 img/sec | O(batch_size) | None |
| Image Embedding | 20-50 img/sec (CPU) | O(batch_size Ã— 2048) | 5-10x |
| Model Training | Varies by model | O(n_features Ã— n_samples) | Model-dependent |
| Ensemble Prediction | 1K-10K samples/sec | O(n_models) | None |

**Scalability Limits:**
- **Text Features**: Up to 1M documents with 50K vocabulary
- **Image Features**: Up to 100K images with batch processing
- **Ensemble Size**: Optimal at 3-7 models, diminishing returns beyond
- **Memory**: Requires 8-16GB RAM for full pipeline with images

---

## ğŸ“š References

* USDA FoodData Central (2023)
* Chawla et al., *SMOTE*, JAI 2002
* He et al., *ResNet*, CVPR 2016
* Salton & Buckley, *TF-IDF*, IR 1988
* RapidFuzz Documentation for fuzzy string matching