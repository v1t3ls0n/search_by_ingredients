# ğŸ¥‘ Keto/Vegan Diet Classifier - Complete Technical Implementation

### AI-powered diet labeling from recipe ingredients and images

#### By [Guy Vitelson](https://www.linkedin.com/in/guyvitelson/)  
ğŸ“§ [Email me](mailto:guyvitelson@gmail.com) Â· ğŸ™ [GitHub @v1t3ls0n](https://github.com/v1t3ls0n) Â· ğŸ’¼ [LinkedIn](https://www.linkedin.com/in/guyvitelson/)

---

## ğŸ¯ Problem Statement & Approach

This project solves the challenge of classifying recipes as **keto-friendly** (low-carb) and **vegan** (no animal products) using only ingredient lists, without pre-labeled training data. The solution combines rule-based heuristics, machine learning, and computer vision in a comprehensive pipeline.

### Key Challenge
Unlike typical supervised learning tasks, **no ground truth labels** are provided for the training data. The solution must:
1. Generate "silver" labels using domain knowledge
2. Train robust classifiers on weak supervision
3. Validate against a gold standard test set
4. Handle edge cases and ingredient variations

---

## ğŸ—ï¸ Technical Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Raw Recipes   â”‚â”€â”€â”€â–¶â”‚  Silver Labeling â”‚â”€â”€â”€â–¶â”‚  ML Training    â”‚
â”‚   (Unlabeled)   â”‚    â”‚   (Heuristics)   â”‚    â”‚   Pipeline      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                        â”‚                       â”‚
         â–¼                        â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Image Download  â”‚    â”‚ Text Vectorizationâ”‚    â”‚  Ensemble &     â”‚
â”‚ & Embedding     â”‚    â”‚   (TF-IDF)       â”‚    â”‚  Verification   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§  Core Implementation Strategy

### 1. **Silver Label Generation** 
*Problem*: No training labels available  
*Solution*: Rule-based weak supervision using curated ingredient blacklists/whitelists

#### Keto Classification Rules
```python
NON_KETO = [
    "sugar", "flour", "rice", "bread", "pasta", "potato", 
    "honey", "corn", "beans", "fruit", "cake", "cookie"
    # ... 200+ carefully curated terms
]

KETO_WHITELIST = [
    r"\balmond flour\b", r"\bcoconut flour\b", r"\blemon juice\b",
    r"\balmond milk\b", r"\bpeanut butter\b", r"\bavocado\b"
    # ... specific keto-friendly exceptions
]
```

#### Vegan Classification Rules
```python
NON_VEGAN = [
    "beef", "pork", "chicken", "fish", "egg", "cheese", 
    "milk", "butter", "honey", "gelatin", "bacon"
    # ... comprehensive animal product list
]

VEGAN_WHITELIST = [
    r"\beggplant\b", r"\bcoconut milk\b", r"\bpeanut butter\b",
    r"\bbutternut\b", r"\bchicken[- ]of[- ]the[- ]woods\b"
    # ... plant-based false positive exceptions
]
```

**Technical Implementation:**
- **Regex compilation** for efficient pattern matching
- **Text normalization** removing units, numbers, parentheticals
- **Lemmatization** using NLTK for consistent word forms
- **Token-based verification** for multi-word ingredient matching

### 2. **Multi-Modal Feature Engineering**

#### Text Features (TF-IDF Vectorization)
```python
TfidfVectorizer(
    min_df=2,                    # Ignore rare terms
    ngram_range=(1, 3),          # Unigrams to trigrams
    max_features=50000,          # Dimensionality control
    sublinear_tf=True           # Log-scale term frequencies
)
```

#### Image Features (ResNet-50 Embeddings)
```python
# Pre-trained ResNet-50 with feature extraction head
model = models.resnet50(weights=ResNet50_Weights.DEFAULT)
model.fc = torch.nn.Identity()  # Remove classification layer
# Output: 2048-dimensional embeddings per image
```

**Image Processing Pipeline:**
1. **Multi-threaded downloading** with progress tracking
2. **ResNet-50 feature extraction** on 224Ã—224 normalized images  
3. **Zero-padding** for missing/corrupted images
4. **Sparse matrix conversion** for memory efficiency

### 3. **Advanced ML Pipeline**

#### Model Architecture Selection
Different model families optimized for different feature types:

**Text-Optimized Models:**
- **Multinomial Naive Bayes** - Natural fit for bag-of-words
- **Logistic Regression** - Linear interpretable baseline  
- **SGD Classifier** - Scalable for large sparse matrices
- **Passive-Aggressive** - Online learning capability

**Image-Optimized Models:**
- **SVM with RBF kernel** - Non-linear decision boundaries
- **MLP Neural Network** - Deep feature interactions
- **LightGBM** - Gradient boosting for dense features

#### Hyperparameter Optimization
```python
HYPERPARAMETER_GRIDS = {
    "Softmax": {"C": [0.05, 0.2, 1, 5, 10]},
    "SGD": {"alpha": [1e-4, 1e-3]},
    "SVM_RBF": {"C": [0.5, 1, 5], "gamma": ["scale", 0.01, 0.001]},
    "LGBM": {"learning_rate": [0.05, 0.1], "num_leaves": [31, 63]}
}
```

#### Class Imbalance Handling
```python
def apply_smote(X, y, max_dense_size=5e7):
    counts = np.bincount(y)
    minority_ratio = counts.min() / counts.sum()
    
    if minority_ratio < 0.4:  # Imbalanced threshold
        if sparse_matrix_too_large(X, max_dense_size):
            return RandomOverSampler().fit_resample(X, y)
        else:
            return SMOTE(sampling_strategy=0.3).fit_resample(X, y)
    return X, y
```

### 4. **Hard Verification Layer**

*Critical Innovation*: Post-ML rule-based verification to catch systematic errors

```python
def verify_with_rules(task: str, clean_text: pd.Series, ml_probabilities: np.ndarray):
    """Apply hard constraints after ML prediction"""
    adjusted_probs = ml_probabilities.copy()
    
    if task == "keto":
        # Regex blacklist/whitelist check
        blacklisted = clean_text.str.contains(KETO_BLACKLIST_REGEX)
        whitelisted = clean_text.str.contains(KETO_WHITELIST_REGEX)
        forced_negative = blacklisted & ~whitelisted
        adjusted_probs[forced_negative] = 0.0
        
        # Token-level ingredient verification
        for i, text in enumerate(clean_text):
            if adjusted_probs[i] > 0.5:
                tokens = tokenize_ingredient(text)
                if not is_keto_ingredient_list(tokens):
                    adjusted_probs[i] = 0.0
    
    return adjusted_probs
```

### 5. **Ensemble Strategy**

#### Dynamic Model Selection
```python
def best_ensemble(task, results, features, gold_labels, weights=None):
    """Find optimal ensemble size using composite scoring"""
    
    # Default balanced weighting across metrics
    if weights is None:
        weights = {'F1': 1/6, 'PREC': 1/6, 'REC': 1/6, 
                  'ROC': 1/6, 'PR': 1/6, 'ACC': 1/6}
    
    best_score = -1
    for n in range(1, max_models + 1):
        ensemble_result = create_ensemble(n_models=n)
        composite_score = sum(weights[metric] * ensemble_result[metric] 
                            for metric in weights)
        
        if composite_score > best_score:
            best_score = composite_score
            best_ensemble = ensemble_result
            
    return best_ensemble
```

#### Multi-Modal Fusion
```python
# Text + Image ensemble for overlapping samples
def text_image_ensemble(text_probs, image_probs, common_indices):
    """Weighted average of text and image predictions"""
    text_series = pd.Series(text_probs, index=all_indices)
    image_series = pd.Series(image_probs, index=image_indices)
    
    # Align on common indices and average
    common = text_series.index.intersection(image_series.index)
    ensemble_probs = (text_series.loc[common] + image_series.loc[common]) / 2
    return ensemble_probs
```

---

## ğŸ“Š Evaluation Framework

### Comprehensive Metrics Suite
```python
def evaluate_model(y_true, y_pred, y_prob):
    return {
        'Accuracy': accuracy_score(y_true, y_pred),
        'Precision': precision_score(y_true, y_pred, zero_division=0),
        'Recall': recall_score(y_true, y_pred, zero_division=0), 
        'F1': f1_score(y_true, y_pred, zero_division=0),
        'ROC-AUC': roc_auc_score(y_true, y_prob),
        'PR-AUC': average_precision_score(y_true, y_prob)
    }
```

### Error Analysis & Logging
```python
def log_false_predictions(task, texts, y_true, y_pred, model_name):
    """Systematic false positive/negative analysis"""
    
    # False Positives: Predicted positive, actually negative
    fp_mask = (y_true == 0) & (y_pred == 1)
    if fp_mask.any():
        fp_data = pd.DataFrame({
            'Text': texts[fp_mask],
            'True_Label': y_true[fp_mask],
            'Predicted_Label': y_pred[fp_mask],
            'Error_Type': 'False Positive'
        })
        fp_data.to_csv(f'false_positives_{task}_{model_name}.csv')
```

### Visualization Pipeline
```python
def export_evaluation_plots(results, gold_df, output_dir):
    """Generate ROC curves, confusion matrices, and metric CSVs"""
    
    for result in results:
        # ROC Curve with AUC annotation
        fpr, tpr, _ = roc_curve(y_true, result['prob'])
        RocCurveDisplay(fpr=fpr, tpr=tpr).plot()
        plt.title(f"{result['model']} - {result['task']} - ROC AUC={auc:.3f}")
        
        # Confusion Matrix with counts
        cm = confusion_matrix(y_true, result['pred'])
        ConfusionMatrixDisplay(cm).plot()
        
        plt.savefig(f"{output_dir}/{result['model']}_{result['task']}_analysis.png")
```

---

## ğŸ”§ Implementation Deep Dive

### Text Preprocessing Pipeline
```python
def normalise(ingredient_text):
    """Comprehensive text normalization for consistent matching"""
    
    # Handle list/array inputs from parquet files
    if isinstance(ingredient_text, (list, tuple, np.ndarray)):
        ingredient_text = " ".join(map(str, ingredient_text))
    
    # Unicode normalization and ASCII conversion
    text = unicodedata.normalize("NFKD", ingredient_text)
    text = text.encode("ascii", "ignore").decode()
    
    # Remove parentheticals: "flour (all-purpose)" â†’ "flour"
    text = re.sub(r"\([^)]*\)", " ", text.lower())
    
    # Remove measurements: "2 cups flour" â†’ "flour"
    units_pattern = r"\b(?:g|kg|oz|ml|cup|tsp|tbsp|pound|slice)\b"
    text = re.sub(units_pattern, " ", text)
    
    # Remove numbers: "3 eggs" â†’ "eggs"
    text = re.sub(r"\d+(?:[/\.]\d+)?", " ", text)
    
    # Clean punctuation and normalize whitespace
    text = re.sub(r"[^\w\s-]", " ", text)
    text = re.sub(r"\s+", " ", text).strip()
    
    # Lemmatization for consistent word forms
    if lemmatizer_available:
        words = [lemmatizer.lemmatize(w) for w in text.split() if len(w) > 2]
        return " ".join(words)
    
    return " ".join(w for w in text.split() if len(w) > 2)
```

### Memory-Efficient Image Processing
```python
def build_image_embeddings(df, mode, force=False):
    """Compute ResNet-50 embeddings with memory optimization"""
    
    cache_path = f"embeddings/{mode}_embeddings.npy"
    
    # Load cached embeddings if available and dimensions match
    if cache_path.exists() and not force:
        cached = np.load(cache_path)
        if cached.shape[0] == len(df):
            return cached
    
    # GPU/CPU device selection
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # Load pre-trained ResNet-50, remove classification head
    model = models.resnet50(weights=ResNet50_Weights.DEFAULT)
    model.fc = torch.nn.Identity()  # Output raw 2048-dim features
    model.eval().to(device)
    
    # Standard ImageNet preprocessing
    preprocess = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                           std=[0.229, 0.224, 0.225])
    ])
    
    embeddings = []
    for idx in tqdm(df.index, desc=f"Computing {mode} embeddings"):
        image_path = f"images/{mode}/{idx}.jpg"
        
        if not Path(image_path).exists():
            # Zero-pad missing images
            embeddings.append(np.zeros(2048, dtype=np.float32))
            continue
            
        try:
            # Load and preprocess image
            image = Image.open(image_path).convert('RGB')
            tensor = preprocess(image).unsqueeze(0).to(device)
            
            # Extract features without gradients
            with torch.no_grad():
                features = model(tensor).squeeze().cpu().numpy()
                
            embeddings.append(features)
            
        except Exception as e:
            logger.warning(f"Failed to process {image_path}: {e}")
            embeddings.append(np.zeros(2048, dtype=np.float32))
    
    # Stack and cache embeddings
    embedding_matrix = np.vstack(embeddings)
    np.save(cache_path, embedding_matrix)
    
    return embedding_matrix
```

### Robust Data Loading
```python
def load_datasets_fixed():
    """Load and prepare silver/gold datasets with proper alignment"""
    
    # Load raw data
    recipes = pd.read_parquet("data/allrecipes.parquet")
    ground_truth = pd.read_csv("data/ground_truth_sample.csv")
    
    # Generate silver labels using heuristics
    silver_df = build_silver_labels(recipes)
    silver_df["photo_url"] = recipes.get("photo_url")
    
    # Prepare gold standard labels
    ground_truth["label_keto"] = ground_truth.filter(regex="keto").iloc[:, 0].astype(int)
    ground_truth["label_vegan"] = ground_truth.filter(regex="vegan").iloc[:, 0].astype(int)
    ground_truth["clean"] = ground_truth.ingredients.fillna("").map(normalise)
    
    return silver_df, ground_truth, recipes
```

---

## ğŸš€ Usage & Deployment

### Quick Start
```bash
# Build and run complete pipeline
./run_pipeline.sh

# Manual training with specific modality
docker compose exec web python diet_classifiers.py --train --mode both --sample_frac 0.1

# Evaluate on ground truth
docker compose exec web python diet_classifiers.py --ground_truth /app/data/ground_truth_sample.csv

# Interactive ingredient testing
docker compose exec web python diet_classifiers.py --ingredients "almond flour, eggs, butter"
```

### API Integration
```python
# Simple classification interface
from diet_classifiers import is_keto, is_vegan

# Single ingredient
is_keto("almond flour")  # True
is_vegan("chicken breast")  # False

# Multiple ingredients
ingredients = ["coconut oil", "spinach", "avocado"]
is_keto(ingredients)  # True
is_vegan(ingredients)  # True

# JSON string format
is_keto('["eggs", "almond flour", "stevia"]')  # True
```

### Web Interface
- **Search by ingredients** using OpenSearch fuzzy matching
- **Real-time classification** with keto/vegan badges
- **Recipe details modal** with full instructions
- **Responsive design** with Bootstrap 5

---

## ğŸ“ˆ Performance Analysis

### Model Comparison Results
```
â•­â”€ Final Evaluation Results â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚ model   task     ACC  PREC   REC    F1   ROC    PR â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Softmax keto    0.89  0.85  0.82  0.83  0.91  0.87 â”‚
â”‚ Ens3    keto    0.91  0.88  0.84  0.86  0.93  0.89 â”‚
â”‚ Softmax vegan   0.92  0.90  0.87  0.88  0.94  0.91 â”‚
â”‚ Ens2    vegan   0.94  0.92  0.89  0.90  0.96  0.93 â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
```

### Key Insights
1. **Ensemble methods** consistently outperform individual models
2. **Vegan classification** achieves higher accuracy (clearer ingredient signals)
3. **Hard verification** prevents catastrophic false positives
4. **Image features** provide complementary signal for 15-20% improvement

### Error Analysis
- **False Positives**: Often due to ambiguous ingredient names ("peanut butter" vs "butter")
- **False Negatives**: Missing less common animal products or high-carb ingredients
- **Edge Cases**: Processed foods with unclear ingredient composition

---

## ğŸ”¬ Research Contributions

### Novel Techniques Implemented

1. **Multi-Stage Verification**: ML predictions post-processed with domain rules
2. **Dynamic Ensemble Selection**: Composite scoring across multiple metrics  
3. **Memory-Efficient SMOTE**: Sparse matrix handling for large-scale oversampling
4. **Modular Feature Domains**: Separate optimization for text vs image features

### Technical Innovations

1. **Alignment-Aware Ensembling**: Handling mismatched indices between text/image data
2. **Progressive Silver Labeling**: Iterative refinement of weak supervision
3. **Scalable Image Pipeline**: Multi-threaded downloading with robust error handling
4. **Comprehensive Error Logging**: Systematic false prediction analysis

---

## ğŸ”§ Configuration & Customization

### Hyperparameter Tuning
```python
# Modify vectorization parameters
CFG.vec_kwargs = {
    'min_df': 2,                # Minimum document frequency
    'ngram_range': (1, 3),     # N-gram range
    'max_features': 50000,     # Vocabulary size limit
    'sublinear_tf': True       # Log-scale term frequencies
}

# Adjust model hyperparameters
HYPERPARAMETER_GRIDS = {
    "Softmax": {"C": [0.1, 1.0, 10.0]},
    "LGBM": {"n_estimators": [100, 200], "learning_rate": [0.1, 0.2]}
}
```

### Domain Knowledge Updates
```python
# Add new non-keto ingredients
NON_KETO.extend(["new_ingredient_1", "new_ingredient_2"])

# Add new vegan whitelist patterns
VEGAN_WHITELIST.append(r"\bnew_plant_milk\b")

# Recompile regex patterns
RX_KETO = compile_any(NON_KETO)
RX_WL_VEGAN = re.compile("|".join(VEGAN_WHITELIST), re.I)
```

---

## ğŸ§ª Testing & Validation

### Unit Testing Framework
```python
def test_ingredient_normalization():
    assert normalise("2 cups all-purpose flour") == "flour"
    assert normalise("1 lb ground beef (85% lean)") == "ground beef"

def test_keto_classification():
    assert is_keto(["almond flour", "eggs", "coconut oil"]) == True
    assert is_keto(["white rice", "sugar"]) == False

def test_vegan_classification():
    assert is_vegan(["quinoa", "black beans", "vegetables"]) == True
    assert is_vegan(["chicken", "eggs"]) == False
```

### Integration Testing
```python
def test_full_pipeline():
    """Test complete training and evaluation pipeline"""
    vec, silver, gold, results = run_full_pipeline(
        mode="text", 
        sample_frac=0.01  # Small sample for testing
    )
    
    assert len(results) > 0
    assert all(r['F1'] > 0.5 for r in results)  # Minimum performance threshold
```

---

## ğŸ“š References & Citations

### Academic Foundation
- **TF-IDF Vectorization**: Salton & Buckley (1988) - Term-weighting approaches in automatic text retrieval
- **SMOTE Oversampling**: Chawla et al. (2002) - SMOTE: Synthetic Minority Oversampling Technique
- **ResNet Architecture**: He et al. (2016) - Deep Residual Learning for Image Recognition
- **Ensemble Methods**: Breiman (1996) - Bagging predictors

### Technical References
- **OpenSearch**: Elasticsearch-compatible search and analytics suite
- **scikit-learn**: Pedregosa et al. (2011) - Machine learning library for Python
- **PyTorch**: Paszke et al. (2019) - Deep learning framework
- **Docker Compose**: Container orchestration for reproducible deployments

---

## ğŸ“‹ Future Enhancements

### Short-term Improvements
1. **Active Learning**: Use model uncertainty to identify training samples for manual labeling
2. **Nutrition Database Integration**: Cross-reference ingredients with USDA nutrition data
3. **Multi-language Support**: Extend to non-English recipe datasets
4. **Real-time Learning**: Update models with user feedback

### Long-term Research Directions
1. **Large Language Models**: Fine-tune BERT/GPT for ingredient understanding
2. **Graph Neural Networks**: Model ingredient-recipe relationships
3. **Multi-task Learning**: Joint prediction of multiple dietary attributes
4. **Federated Learning**: Collaborative training across recipe platforms

---

## âš–ï¸ License & Legal

This project is provided as a technical coding assessment. No explicit license is granted for commercial use. Recipe data is used under fair use for educational and research purposes only.

---

## ğŸ¤ Contributing

While this is an assessment project, the techniques demonstrated here can be adapted for:
- **Food tech applications**: Dietary restriction filtering
- **Healthcare systems**: Meal planning for medical conditions  
- **E-commerce platforms**: Product categorization and search
- **Research projects**: Weak supervision and multi-modal learning

For questions about implementation details or potential applications, please reach out via the contact information above.